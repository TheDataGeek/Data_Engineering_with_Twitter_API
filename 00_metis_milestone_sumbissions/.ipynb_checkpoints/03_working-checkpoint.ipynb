{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd022ccc-449d-47c3-ad17-a51d88caac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install vaderSentiment\n",
    "# pip install gensim\n",
    "# pip install wordcloud\n",
    "# pip install pyldavis\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2009ae7-4296-42ce-baa2-c397f076626b",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3641c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "# eda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data handling\n",
    "import pyspark \n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# topic modeling\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# misc\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b871d5-9c7f-4789-b6a2-0dcebffd5e90",
   "metadata": {},
   "source": [
    "# instantiate PySpark SparkSession and import JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef2e94c-2410-48fa-b61c-1c9ff0e0b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1764439-2927-48b9-a403-3a9049082632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_9885/717904347.py\", line 1, in <cell line: 1>\n",
      "    df = spark.read.json('../01_data_collection/*.json')\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 284, in json\n",
      "    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../01_data_collection/*.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:284\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2004\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2004\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:538\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    532\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    533\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    535\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    539\u001b[0m }\n\u001b[1;32m    541\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../01_data_collection/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59c043-bc5e-4600-8f95-5029742f374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720174bf-fd20-4d86-9186-00ee38491469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280702a-4dd8-4c0f-8b40-732c9ed40d51",
   "metadata": {},
   "source": [
    "# perform basic SQL queries of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e99aa-ed0b-4194-84f5-89d9b279cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return count of all tweets\n",
    "total_tweets = spark.sql('''\n",
    "                                SELECT\n",
    "                                    id_str as tweet_unique_id\n",
    "                                FROM\n",
    "                                    df\n",
    "                                                            ''')\n",
    "\n",
    "total_tweets.select(countDistinct('tweet_unique_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75881eeb-a0f3-4ded-8ca7-b731f6873318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return count of all tweets\n",
    "total_tweets = spark.sql('''\n",
    "                                SELECT\n",
    "                                    id_str as tweet_unique_id\n",
    "                                FROM\n",
    "                                    df\n",
    "                                                            ''')\n",
    "\n",
    "total_tweets.select(countDistinct('tweet_unique_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c008c2-d830-49f4-a1a8-b6c5851a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return count of unique twitter users:\n",
    "unique_users = spark.sql('''\n",
    "                                SELECT\n",
    "                                    user.id_str as user_unique_id\n",
    "                                FROM\n",
    "                                    df\n",
    "                                                             ''')\n",
    "\n",
    "unique_users.select(countDistinct('user_unique_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339ae0f-2bce-489e-befc-889bb63ff775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return count of unique tweets (not including retweets):\n",
    "unique_tweets = spark.sql('''\n",
    "                                SELECT\n",
    "                                    id_str as tweet_unique_id\n",
    "                                FROM\n",
    "                                    df\n",
    "                                WHERE \n",
    "                                    retweeted_status IS NULL\n",
    "                                                                    ''')\n",
    "\n",
    "unique_tweets.select(countDistinct('tweet_unique_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8c5cb-77ef-45b6-b35c-ee8b4f5bf7af",
   "metadata": {},
   "source": [
    "# deeper dive with SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40b67e-4bfd-434a-af7f-4442cc6d82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tweet user metadata\n",
    "# user = spark.sql('''\n",
    "#                     SELECT \n",
    "#                         id_str as original_tweet_unique_id,\n",
    "#                         user.id_str as user_id,\n",
    "#                         user.screen_name as user_twitter_handle,\n",
    "#                         user.description as user_description,\n",
    "#                         user.name as user_display_name,\n",
    "#                         user.location as user_location,\n",
    "#                         user.followers_count as user_followers_count,\n",
    "#                         user.friends_count as user_friends_count,\n",
    "#                         user.statuses_count as user_tweets_count\n",
    "#                     FROM \n",
    "#                         df\n",
    "#                             ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666bc2d-cd07-480e-ba86-cb05555a55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retweeted user metadata\n",
    "# retweeted = spark.sql('''\n",
    "#                         SELECT\n",
    "#                             id_str as original_tweet_unique_id,\n",
    "#                             retweeted_status as retweet_metadata,\n",
    "#                             retweeted_status.full_text as retweeted_tweet_text,\n",
    "#                             retweeted_status.id_str as retweeted_unique_id,\n",
    "#                             retweeted_status.user.id_str as retweeted_user_id,\n",
    "#                             retweeted_status.user.screen_name as retweeted_user_twitter_handle,\n",
    "#                             retweeted_status.user.name as retweeted_user_display_name,\n",
    "#                             retweeted_status.user.location as retweeted_user_location,\n",
    "#                             retweeted_status.user.followers_count as retweeted_user_followers_count,\n",
    "#                             retweeted_status.user.friends_count as retweeeted_user_friends_count,\n",
    "#                             retweeted_status.user.statuses_count as retweeted_user_tweets_count\n",
    "#                         FROM\n",
    "#                             (SELECT *\n",
    "#                              FROM df\n",
    "#                              WHERE retweeted_status IS NOT NULL) as retweeted_subquery\n",
    "#                                                                                         ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6c83f-f235-4ca6-8153-80787321e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quoted tweet user metadata\n",
    "# quoted = spark.sql('''\n",
    "#                     SELECT\n",
    "#                         id_str as original_tweet_unique_id,\n",
    "#                         is_quote_status as quoted_tweet,\n",
    "#                         quoted_status as quoted_tweet_metadata,\n",
    "#                         quoted_status.id_str as quoted_tweet_unique_id,\n",
    "#                         quoted_status.user.id_str as quoted_tweet_user_id,\n",
    "#                         quoted_status.user.screen_name as quoted_user_twitter_handle,\n",
    "#                         quoted_status.user.name as quoted_user_display_name,\n",
    "#                         quoted_status.user.location as quoted_user_location,\n",
    "#                         quoted_status.user.followers_count as quoted_user_followers_count,\n",
    "#                         quoted_status.user.friends_count as quoted_user_friends_count,\n",
    "#                         quoted_status.user.statuses_count as quoted_user_tweets_count\n",
    "#                     FROM\n",
    "#                         (SELECT * \n",
    "#                          FROM df\n",
    "#                          WHERE quoted_status IS NOT NULL) as quoted_subquery\n",
    "#                                                                                 ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d97dec-dcb8-4bb4-8cd1-d517ab4ecfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tweet relevancy for climate change\n",
    "sample_climate_tweets = spark.sql('''\n",
    "                                    SELECT \n",
    "                                        full_text as tweet_text\n",
    "                                    FROM\n",
    "                                        df\n",
    "                                     WHERE\n",
    "                                         full_text like '%climate change%'\n",
    "                                                                            ''').take(10)\n",
    "\n",
    "sample_climate_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edc7cc-64dd-46cd-b13a-4c687adab1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tweet relevancy for global warming\n",
    "sample_global_tweets = spark.sql('''\n",
    "                                    SELECT \n",
    "                                        full_text as tweet_text\n",
    "                                    FROM\n",
    "                                        df\n",
    "                                     WHERE\n",
    "                                         full_text like '%global warming%'\n",
    "                                                                        ''').take(10)\n",
    "\n",
    "sample_global_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a8ec9-845c-4f73-8088-ae2b81aef790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tweet relevancy for climate deniers\n",
    "sample_denier_tweets = spark.sql('''\n",
    "                                    SELECT \n",
    "                                        full_text as tweet_text\n",
    "                                    FROM\n",
    "                                        df\n",
    "                                     WHERE\n",
    "                                         full_text like '%climate deniers%'\n",
    "                                                                        ''').take(10)\n",
    "\n",
    "sample_denier_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334add41-5b28-4250-8664-72cd21768f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most retweeted\n",
    "most_retweeted = spark.sql('''\n",
    "                                SELECT\n",
    "                                    user.screen_name as user_twitter_handle,\n",
    "                                    full_text as tweet_text,\n",
    "                                    SUM(retweet_count) as total_retweets\n",
    "                                FROM\n",
    "                                    df\n",
    "                                WHERE\n",
    "                                    retweeted_status IS NULL\n",
    "                                GROUP BY\n",
    "                                    user_twitter_handle,\n",
    "                                    tweet_text\n",
    "                                ORDER BY\n",
    "                                    total_retweets DESC\n",
    "                                LIMIT 20\n",
    "                                                        ''')\n",
    "\n",
    "most_retweeted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbcc8e-007b-40d9-bd1a-fc1b6b67fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted_df = most_retweeted.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588087d6-97ce-4977-88ae-0669bd9bfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted_df['tweet_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f754c-be2a-457a-8521-dfed46509a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted_df['total_retweets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bee85e-f0bb-4cd6-8aab-feb7e4d26f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted_df['user_twitter_handle'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ccc37-0b9e-4549-a36d-ee3b6ebe0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users with most followers\n",
    "most_followers = spark.sql('''\n",
    "                                SELECT\n",
    "                                    user.screen_name as user_twitter_handle,\n",
    "                                    MAX(user.followers_count) as max_user_followers\n",
    "                                FROM\n",
    "                                    df\n",
    "                                GROUP BY\n",
    "                                    user_twitter_handle\n",
    "                                ORDER BY\n",
    "                                    max_user_followers DESC\n",
    "                                LIMIT 20\n",
    "                                                        ''')\n",
    "\n",
    "most_followers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec9756-25b5-4d9f-8fb9-4ce2ce027d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_followers_df = most_followers.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229effe0-dbb8-406f-a03b-a27e8bf21acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_followers_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6caab-d9de-41a4-a8cd-70dd5f7b095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_user_mentions\n",
    "user_mentions = spark.sql('''\n",
    "                            SELECT\n",
    "                                entities.user_mentions.screen_name as tweet_mentioned_user_twitter_handle\n",
    "                            FROM\n",
    "                                df\n",
    "                                    ''')\n",
    "\n",
    "user_mentions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858e204-5184-4392-b365-a362213d5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0cc9a1-3273-4e4b-ade3-edef30affa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions_list = []\n",
    "for mentions in user_mentions.collect():\n",
    "    for obj in mentions['tweet_mentioned_user_twitter_handle']:\n",
    "        user_mentions_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06675a20-bb6d-4784-97a2-bdcc7d8d3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff05e20-f7ef-425f-994f-0ed18ce17e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_mentions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356d163-8b52-43d7-90ed-5545c68094d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_mentions = Counter(user_mentions_list).most_common(20)\n",
    "most_common_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed3a5a-8a40-4dc8-bd1f-0d8185e7cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_user_mentions = pd.DataFrame(most_common_mentions, columns=[['user_name','mentions']])\n",
    "most_user_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb575b7-b4ed-4ad8-b78a-08baf84ae50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from most common location\n",
    "user_location = spark.sql('''\n",
    "                            SELECT\n",
    "                                user.location as user_location\n",
    "                            FROM\n",
    "                                df\n",
    "                            WHERE \n",
    "                                retweeted_status.id_str IS NULL    \n",
    "                                                                ''')\n",
    "\n",
    "user_location.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad332e-e877-4f1a-b479-b34fc6865a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6ecec-a239-4cab-8bf8-33255d933dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location_list = []\n",
    "for location in user_location.collect():\n",
    "    user_location_list.append(location['user_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8f788-1397-4db1-957a-3864caa1aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b36369-bfb9-4c99-9254-655328f8427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_location_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eef262-1511-4d9d-a781-608ca5dacf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_locations = Counter(user_location_list).most_common(20)\n",
    "most_common_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086cc12-1eda-471a-83fc-64ac7cdb9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_user_locations = pd.DataFrame(most_common_locations, columns=[['location','count']])\n",
    "most_user_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879d7a4-ad2e-4a42-a2cc-162d21911f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet metadata\n",
    "tweets = spark.sql('''\n",
    "                        SELECT\n",
    "                            id_str as tweet_unique_id,\n",
    "                            lang as tweet_lang,\n",
    "                            full_text as tweet_text\n",
    "                        FROM\n",
    "                            df\n",
    "                        WHERE \n",
    "                            retweeted_status.id_str IS NULL\n",
    "                                                            ''')\n",
    "\n",
    "tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877690aa-a146-4045-8ddc-0c57ab5c449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c477db-5680-48e4-9030-abc925b22dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be55ca2-3099-403f-b077-0668a6b09295",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2c4d4-a69c-4635-8ba6-98ca895cd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 'tweet_lang' unique count\n",
    "tweets_df['tweet_lang'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb414e-8873-454e-bdb2-36fdca1f5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 'tweet_lang' values\n",
    "tweets_df['tweet_lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cca0f3-b595-4893-b5ac-bdd8966a5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'tweet_lang' != 'en'\n",
    "tweets_df = tweets_df[tweets_df['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cfaf6-184b-4443-af36-922a9f10b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = tweets_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d773b4d-f58e-4d3d-a8d1-b0a06ad99992",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e7cce-86f4-4512-8a90-ee573c780e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet[0] before preprocessing for reference\n",
    "sample_tweets['tweet_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc87110-9a4b-4f71-ad4a-473f9fff8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "sample_tweets['converted_tweets'] = sample_tweets['tweet_text'].apply(lambda x: re.sub('htt\\S+', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef596d-f963-4667-9231-db92851f069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and lowercase text\n",
    "sample_tweets['converted_tweets'] = sample_tweets['converted_tweets'].apply(lambda x: re.sub('[^\\w\\s]', ' ', x)).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1b0b5-314c-4dcb-8cd9-0f6f127aa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet[0] after preprocessing\n",
    "sample_tweets['converted_tweets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c699b-956d-483f-9978-7b4598f2bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3ce63-f953-4b82-851a-69604924d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply method to cleaned text, add new column\n",
    "sample_tweets['score'] = sample_tweets['converted_tweets'].apply(analyzer.polarity_scores).apply(lambda x: x.get('compound'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01a5f3-5ff3-41ee-a536-2b2df97ccc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977192c-244f-4068-b5a7-a3b2c60d8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets['sentiment'] = sample_tweets['score'].apply(lambda x: 'Positive' if x > 0.0 else 'Negative' if x < 0.0 else 'Neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec5362-d9cb-4e77-8b6d-0ca98e148e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3144b02-d54b-41f8-a0b9-b2382a2a7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.title('Tweet Sentiment Distribution',fontsize = 20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "ax = sns.countplot(x=sample_tweets['sentiment'])\n",
    "ax.set(xlabel=None)\n",
    "ax.set(ylabel=None)\n",
    "\n",
    "# plt.savefig('../04_data_visualizations/tweet_sentiment_distribution.png', transparent=True)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9744ce-cdf5-4381-bdf5-e062feaaad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('http')\n",
    "stop_words.add('https')\n",
    "stop_words.add('co')\n",
    "stop_words.add('com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d54bf-4f57-4fd5-8db3-c108153cde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sample_tweets['tweet_text']\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2b01c-978c-4a7d-ac76-8f40b430970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing function using spaCy.lemma_ and spaCy.pos_\n",
    "def lemmatization(texts, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        lemmatized_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                lemmatized_text.append(token.lemma_)\n",
    "        final = ' '.join(lemmatized_text)\n",
    "        processed_texts.append(final)\n",
    "    return (processed_texts)\n",
    "\n",
    "lemmatized_texts = lemmatization(corpus)\n",
    "lemmatized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e96034-d6ae-4693-97ca-01c50ff38851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional text preprocessing function using gensim.utils.simple_preprocess\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return(final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "data_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cea348-3a88-472f-9b58-e3a94dc5c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f1930-6970-4477-add7-4ab367c56239",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:', len(corpus))\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "# Filter out words that occur less than 500 documents, or more than 25% of the documents.\n",
    "dictionary.filter_extremes(no_below=500, no_above=0.25)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754ba2f-215f-4eb7-95c0-4fdae6a967e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform processed words into str for wordcloud\n",
    "strings = []\n",
    "\n",
    "for doc in data_words:\n",
    "    for token in doc:\n",
    "        strings.append(token)\n",
    "\n",
    "words = ' '.join(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7f5d0-5b58-4be3-afe3-0fcacbbc687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wordcloud object\n",
    "wordcloud = WordCloud(font_path='../05_misc/Trebuchet MS Bold.TTF', \n",
    "                      stopwords=stop_words, \n",
    "                      width=1920, height=1080, \n",
    "                      max_words=100, colormap='Blues')\n",
    "\n",
    "wordcloud.generate(words)\n",
    "\n",
    "plt.figure(figsize= (15,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "# plt.savefig('../04_data_visualizations/tweet_word_cloud.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e412ccf-2c69-44e4-8615-b5937ef231b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dictionary\n",
    "\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e34406-2d69-491d-a4f9-bcb451d648f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=num_topics,\n",
    "                     random_state=42,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=20,\n",
    "                     alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86b6d3-9a62-48ab-bfba-db1655874f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562730c7-9532-4cdd-ae4d-8c961c7c8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_words for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.20); ax.set_ylim(0, 25000)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper right'); ax_twin.legend(loc='upper left')\n",
    "\n",
    "fig.tight_layout(w_pad=5)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)  \n",
    "plt.savefig('../04_data_visualizations/tweet_keywords_by_topic.png', transparent=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cefe3-2b2a-4698-a333-1214fa2d2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d8143-ce77-4fb1-9630-b3b13187e4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
