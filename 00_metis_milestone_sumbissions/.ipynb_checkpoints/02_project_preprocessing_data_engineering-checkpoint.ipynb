{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE SEE METIS MILESTONE SUBMISSIONS FOR NOTEBOOKS WITH EXECUTED CODE\n",
    "# THIS NOTEBOOK COMPILES CODE FROM DATA ACQUISTION, EDA, PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b43fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015dc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ACQUISITION NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5208b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy, logging, json, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \n",
    "API_SECRET = \n",
    "BEARER_TOKEN = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942608fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, \n",
    "                 parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "if (not api):\n",
    "    print('Failed Authentication')\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinceId = None\n",
    "max_id = None\n",
    "maxTweets = 2000000\n",
    "tweetsPerQry = 100\n",
    "tweetCount = 0\n",
    "tweetFiles = 0\n",
    "tweets = []\n",
    "searchCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchQuery = \"abortion OR abortions OR (abortion AND 'pro-birth') OR (abortion AND 'pro-choice') OR (abortion AND'pro-life') OR (abortions AND 'pro-birth') OR (abortions AND 'pro-choice') OR (abortions AND'pro-life') OR (roe AND wade)\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while 10000*tweetFiles + tweetCount < maxTweets:\n",
    "    try:\n",
    "        if (not max_id):\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search_tweets(q=searchQuery, \n",
    "                                               tweet_mode='extended',\n",
    "                                               count=tweetsPerQry)\n",
    "        \n",
    "            else:\n",
    "                new_tweets = api.search_tweets(q=searchQuery, \n",
    "                                               tweet_mode='extended',\n",
    "                                               count = tweetsPerQry, \n",
    "                                               since_id = sinceId)\n",
    "\n",
    "        else: \n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search_tweets(q=searchQuery, \n",
    "                                               tweet_mode='extended',\n",
    "                                               count= tweetsPerQry, \n",
    "                                               max_id=str(max_id -1))\n",
    "\n",
    "            else:\n",
    "                new_tweets = api.search_tweets(q=searchQuery,\n",
    "                                               tweet_mode='extended',\n",
    "                                               count=tweetsPerQry, \n",
    "                                               max_id=str(max_id -1), \n",
    "                                               since_id=sinceId)    \n",
    "\n",
    "        new_tweets = new_tweets['statuses']\n",
    "            \n",
    "        if len(new_tweets) == 0:\n",
    "            msg = 'No remaining tweets found'\n",
    "            logging.error(msg)\n",
    "            break\n",
    "        \n",
    "        tweets.extend(new_tweets)\n",
    "        tweetCount += len(new_tweets)\n",
    "        msg = f'{tweetCount} tweets downloaded'\n",
    "        logging.error(msg)\n",
    "        max_id = new_tweets[-1]['id']\n",
    "    \n",
    "        if tweetCount >= 10000:\n",
    "            with open('../01_data_collection/tweets'+str(tweetFiles)+'.json', 'w') as outfile:\n",
    "                json.dump(tweets, outfile)\n",
    "            msg = 'JSON file saved'\n",
    "            logging.error(msg)\n",
    "            tweetFiles += 1\n",
    "            tweetCount = 0\n",
    "            del tweets[:]\n",
    "        \n",
    "    except tweepy.errors.TweepyException as e:\n",
    "        msg = f'Query failed at max_id {max_id} : {e}'\n",
    "        logging.error(msg)\n",
    "\n",
    "if tweetCount > 0:\n",
    "    with open('../01_data_collection/tweets'+str(tweetFiles)+'.json', 'w') as outfile:\n",
    "        json.dump(tweets, outfile)\n",
    "    msg = 'JSON file saved'\n",
    "    logging.error(msg)\n",
    "    tweetFiles += 1\n",
    "    tweetCount = 0\n",
    "    del tweets[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../01_data_collection/tweets0.json') as json_file:\n",
    "    data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_observations = 0\n",
    "for num in range(0,63):\n",
    "    with open(f'../01_data_collection/tweets{num}.json') as json_file:\n",
    "        data=json.load(json_file)\n",
    "    total_observations += len(data)\n",
    "    print(total_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913489b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6235a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# data handling\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "\n",
    "# nlp eda\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ed7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f00293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('../01_data_collection/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac154cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df82e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695218bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tweet user metadata\n",
    "# user = spark.sql('''\n",
    "#                     SELECT \n",
    "#                         id_str as original_tweet_unique_id,\n",
    "#                         user.id_str as user_id,\n",
    "#                         user.screen_name as user_twitter_handle,\n",
    "#                         user.description as user_description,\n",
    "#                         user.name as user_display_name,\n",
    "#                         user.location as user_location,\n",
    "#                         user.followers_count as user_followers_count,\n",
    "#                         user.friends_count as user_friends_count,\n",
    "#                         user.statuses_count as user_tweets_count\n",
    "#                     FROM \n",
    "#                         df\n",
    "#                             ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecde80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retweeted user metadata\n",
    "# retweeted = spark.sql('''\n",
    "#                         SELECT\n",
    "#                             id_str as original_tweet_unique_id,\n",
    "#                             retweeted_status as retweet_metadata,\n",
    "#                             retweeted_status.full_text as retweeted_tweet_text,\n",
    "#                             retweeted_status.id_str as retweeted_unique_id,\n",
    "#                             retweeted_status.user.id_str as retweeted_user_id,\n",
    "#                             retweeted_status.user.screen_name as retweeted_user_twitter_handle,\n",
    "#                             retweeted_status.user.name as retweeted_user_display_name,\n",
    "#                             retweeted_status.user.location as retweeted_user_location,\n",
    "#                             retweeted_status.user.followers_count as retweeted_user_followers_count,\n",
    "#                             retweeted_status.user.friends_count as retweeeted_user_friends_count,\n",
    "#                             retweeted_status.user.statuses_count as retweeted_user_tweets_count\n",
    "#                         FROM\n",
    "#                             (SELECT *\n",
    "#                              FROM df\n",
    "#                              WHERE retweeted_status IS NOT NULL) as retweeted_subquery\n",
    "#                                                                                         ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38010277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quoted tweet user metadata\n",
    "# quoted = spark.sql('''\n",
    "#                     SELECT\n",
    "#                         id_str as original_tweet_unique_id,\n",
    "#                         is_quote_status as quoted_tweet,\n",
    "#                         quoted_status as quoted_tweet_metadata,\n",
    "#                         quoted_status.id_str as quoted_tweet_unique_id,\n",
    "#                         quoted_status.user.id_str as quoted_tweet_user_id,\n",
    "#                         quoted_status.user.screen_name as quoted_user_twitter_handle,\n",
    "#                         quoted_status.user.name as quoted_user_display_name,\n",
    "#                         quoted_status.user.location as quoted_user_location,\n",
    "#                         quoted_status.user.followers_count as quoted_user_followers_count,\n",
    "#                         quoted_status.user.friends_count as quoted_user_friends_count,\n",
    "#                         quoted_status.user.statuses_count as quoted_user_tweets_count\n",
    "#                     FROM\n",
    "#                         (SELECT * \n",
    "#                          FROM df\n",
    "#                          WHERE quoted_status IS NOT NULL) as quoted_subquery\n",
    "#                                                                                 ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ba08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most retweeted tweet\n",
    "most_retweeted = spark.sql('''\n",
    "                                SELECT\n",
    "                                    user.screen_name as user_twitter_handle,\n",
    "                                    full_text as tweet_text,\n",
    "                                    SUM(retweet_count) as total_retweets\n",
    "                                FROM\n",
    "                                    df\n",
    "                                WHERE\n",
    "                                    retweeted_status.id_str IS NULL\n",
    "                                GROUP BY\n",
    "                                    user_twitter_handle,\n",
    "                                    tweet_text\n",
    "                                ORDER BY\n",
    "                                    total_retweets DESC\n",
    "                                LIMIT 20\n",
    "                                                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted_df = most_retweeted.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0113a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_retweeted_df['tweet_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c720f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users with most followers\n",
    "most_followers = spark.sql('''\n",
    "                                SELECT\n",
    "                                    user.screen_name as user_twitter_handle,\n",
    "                                    MAX(user.followers_count) as max_user_followers\n",
    "                                FROM\n",
    "                                    df\n",
    "                                GROUP BY\n",
    "                                    user_twitter_handle\n",
    "                                ORDER BY\n",
    "                                    max_user_followers DESC\n",
    "                                LIMIT 20\n",
    "                                                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_followers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c325ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_followers_df = most_followers.toPandas()\n",
    "most_followers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_user_mentions\n",
    "user_mentions = spark.sql('''\n",
    "            SELECT\n",
    "                entities.user_mentions.screen_name as tweet_mentioned_user_twitter_handle\n",
    "            FROM\n",
    "                df\n",
    "                    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b208fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions_df = user_mentions.toPandas()\n",
    "user_mentions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1270b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions_list = []\n",
    "for mentions in user_mentions_df['tweet_mentioned_user_twitter_handle']:\n",
    "    for obj in mentions:\n",
    "        user_mentions_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c80312",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_mentions = Counter(user_mentions_list).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af56921",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(most_common_mentions, columns=[['user_name','mentions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from most common location\n",
    "location = spark.sql('''\n",
    "                        SELECT\n",
    "                            user.location as user_location\n",
    "                        FROM\n",
    "                            df\n",
    "                        WHERE \n",
    "                            retweeted_status.id_str IS NULL    \n",
    "                                                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0086f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = location.toPandas()\n",
    "location_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd137cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_list =[]\n",
    "for location in location_df['user_location']:\n",
    "    location_list.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_location = Counter(location_list).most_common(30)\n",
    "most_common_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet metadata\n",
    "tweets = spark.sql('''\n",
    "                        SELECT\n",
    "                            id_str as tweet_unique_id,\n",
    "                            lang as tweet_lang,\n",
    "                            full_text as tweet_text\n",
    "                        FROM\n",
    "                            df\n",
    "                        WHERE \n",
    "                            retweeted_status.id_str IS NULL\n",
    "                                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cd434",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 'tweet_lang' unique count\n",
    "tweets_df['tweet_lang'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efa79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 'tweet_lang' values\n",
    "tweets_df['tweet_lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'tweet_lang' != 'en'\n",
    "tweets_df = tweets_df[tweets_df['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = tweets_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52127563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet[0] before preprocessing for reference\n",
    "sample_tweets['tweet_text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc971bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "sample_tweets['converted_tweets'] = sample_tweets['tweet_text'].apply(lambda x: re.sub('htt\\S+', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and lowercase text\n",
    "sample_tweets['converted_tweets'] = sample_tweets['converted_tweets'].apply(lambda x: re.sub('[^\\w\\s]', ' ', x)).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet[0] after preprocessing\n",
    "sample_tweets['converted_tweets'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply method to cleaned text, add new column\n",
    "sample_tweets['score'] = sample_tweets['converted_tweets'].apply(analyzer.polarity_scores).apply(lambda x: x.get('compound'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ff982",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets['sentiment'] = sample_tweets['score'].apply(lambda x: 'Positive' if x > 0.0 else 'Negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed071c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "sns.countplot(sample_tweets['sentiment'])\n",
    "labels = ['Positive', 'Negative']\n",
    "plt.title('Tweet Sentiment Distribution',fontsize = 16)\n",
    "plt.xticks(range(2), labels) \n",
    "plt.xlabel('sentiment type', fontsize =14)\n",
    "plt.ylabel('frequency', fontsize = 14)\n",
    "# plt.savefig('../04_data_visualizations/tweet_sentiment_distribution.png', transparent=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6489584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa477ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd72326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "\n",
    "# nlp \n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# topic modeling\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# misc\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install vaderSentiment\n",
    "# pip install gensim\n",
    "# pip install wordcloud\n",
    "# pip install pyldavis\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eae9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('../01_data_collection/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9df931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e13f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet metadata\n",
    "tweets = spark.sql('''\n",
    "                        SELECT\n",
    "                            id_str as tweet_unique_id,\n",
    "                            lang as tweet_lang,\n",
    "                            full_text as tweet_text\n",
    "                        FROM\n",
    "                            df\n",
    "                        WHERE \n",
    "                            retweeted_status.id_str IS NULL\n",
    "                                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[tweets_df['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = tweets_df.copy()\n",
    "data = sample_tweets['tweet_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('http')\n",
    "stopwords.append('https')\n",
    "stopwords.append('co')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34346c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sample_tweets['tweet_text']\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing function using spaCy.lemma_ and spaCy.pos_\n",
    "def lemmatization(texts, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        lemmatized_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                lemmatized_text.append(token.lemma_)\n",
    "        final = ' '.join(lemmatized_text)\n",
    "        processed_texts.append(final)\n",
    "    return (processed_texts)\n",
    "\n",
    "lemmatized_texts = lemmatization(corpus)\n",
    "lemmatized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional text preprocessing function using gensim.utils.simple_preprocess\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return(final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "data_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=200, no_above=0.20)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bad24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=num_topics,\n",
    "                     random_state=42,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=20,\n",
    "                     alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform processed words into str for wordcloud\n",
    "strings = []\n",
    "\n",
    "for doc in data_words:\n",
    "    for token in doc:\n",
    "        strings.append(token)\n",
    "\n",
    "words = ' '.join(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wordcloud object\n",
    "wordcloud = WordCloud(font_path='../05_misc/Trebuchet MS Bold.TTF', stopwords=stopwords, width=1920, height=1080, \n",
    "                      background_color=\"black\", max_words=100, contour_width=3, \n",
    "                      colormap='Blues', random_state=42)\n",
    "\n",
    "wordcloud.generate(words)\n",
    "\n",
    "plt.figure(figsize= (15,10))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"100 Most Common Twitter Words\", pad = 14, weight = 'bold')\n",
    "# plt.savefig('../04_data_visualizations/tweet_word_cloud.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_words for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.1); ax.set_ylim(0, 15000)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper right'); ax_twin.legend(loc='center right')\n",
    "\n",
    "fig.tight_layout(w_pad=5)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)  \n",
    "plt.savefig('../04_data_visualizations/tweet_keywords_by_topic.png', transparent=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07732b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # text preprocessing function using WordNetLemmatizer()\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# def docs_preprocessor(docs):\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     for idx in range(len(docs)):\n",
    "#         docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "#         docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "#     # Remove numbers, but not words that contain numbers.\n",
    "#     docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "#     # Remove words that are only one character.\n",
    "#     docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "#     # Lemmatize all words in documents.\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "#     return docs\n",
    "\n",
    "# docs = docs_preprocessor(corpus)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "\n",
    "# # Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "# bigram = Phrases(docs, min_count=10)\n",
    "# trigram = Phrases(bigram[docs])\n",
    "\n",
    "# for idx in range(len(docs)):\n",
    "#     for token in bigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             docs[idx].append(token)\n",
    "#     for token in trigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a trigram, add to document.\n",
    "#             docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5003fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# dictionary = Dictionary(docs)\n",
    "\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import LdaModel\n",
    "\n",
    "# # Set training parameters.\n",
    "# num_topics = 4\n",
    "# chunksize = 500 # size of the doc looked at every pass\n",
    "# passes = 20 # number of passes through documents\n",
    "# iterations = 400\n",
    "# eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# # Make a index to word dictionary.\n",
    "# temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "# id2word = dictionary.id2token\n",
    "\n",
    "# %time \n",
    "\n",
    "# model = LdaModel(corpus=corpus, id2word=id2word, \n",
    "#                  chunksize=chunksize, alpha='auto', \n",
    "#                  eta='auto', iterations=iterations, \n",
    "#                  num_topics=num_topics, passes=passes, \n",
    "#                  eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e06d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim_models \n",
    "# pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.gensim_models.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a8476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276a34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2ade6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
