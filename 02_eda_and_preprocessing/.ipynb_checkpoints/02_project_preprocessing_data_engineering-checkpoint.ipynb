{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f0a8f2-d2c9-4508-9274-f4dce9f9d3f1",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "\n",
    "# nlp \n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# topic modeling\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# misc\n",
    "from pprint import pprint\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install vaderSentiment\n",
    "# pip install gensim\n",
    "# pip install wordcloud\n",
    "# pip install pyldavis\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eae9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('../01_data_collection/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9df931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e13f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet metadata\n",
    "tweets = spark.sql('''\n",
    "                        SELECT\n",
    "                            id_str as tweet_unique_id,\n",
    "                            lang as tweet_lang,\n",
    "                            full_text as tweet_text\n",
    "                        FROM\n",
    "                            df\n",
    "                        WHERE \n",
    "                            retweeted_status.id_str IS NULL\n",
    "                                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[tweets_df['tweet_lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = tweets_df.copy()\n",
    "data = sample_tweets['tweet_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('http')\n",
    "stopwords.append('https')\n",
    "stopwords.append('co')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34346c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sample_tweets['tweet_text']\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing function using spaCy.lemma_ and spaCy.pos_\n",
    "def lemmatization(texts, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        lemmatized_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                lemmatized_text.append(token.lemma_)\n",
    "        final = ' '.join(lemmatized_text)\n",
    "        processed_texts.append(final)\n",
    "    return (processed_texts)\n",
    "\n",
    "lemmatized_texts = lemmatization(corpus)\n",
    "lemmatized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional text preprocessing function using gensim.utils.simple_preprocess\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return(final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "data_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=200, no_above=0.20)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bad24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=num_topics,\n",
    "                     random_state=42,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=20,\n",
    "                     alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform processed words into str for wordcloud\n",
    "strings = []\n",
    "\n",
    "for doc in data_words:\n",
    "    for token in doc:\n",
    "        strings.append(token)\n",
    "\n",
    "words = ' '.join(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wordcloud object\n",
    "wordcloud = WordCloud(font_path='../05_misc/Trebuchet MS Bold.TTF', stopwords=stopwords, width=1920, height=1080, \n",
    "                      background_color=\"black\", max_words=100, contour_width=3, \n",
    "                      colormap='Blues', random_state=42)\n",
    "\n",
    "wordcloud.generate(words)\n",
    "\n",
    "plt.figure(figsize= (15,10))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"100 Most Common Twitter Words\", pad = 14, weight = 'bold')\n",
    "# plt.savefig('../04_data_visualizations/tweet_word_cloud.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_words for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.1); ax.set_ylim(0, 15000)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper right'); ax_twin.legend(loc='center right')\n",
    "\n",
    "fig.tight_layout(w_pad=5)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)  \n",
    "plt.savefig('../04_data_visualizations/tweet_keywords_by_topic.png', transparent=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07732b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # text preprocessing function using WordNetLemmatizer()\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# def docs_preprocessor(docs):\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     for idx in range(len(docs)):\n",
    "#         docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "#         docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "#     # Remove numbers, but not words that contain numbers.\n",
    "#     docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "#     # Remove words that are only one character.\n",
    "#     docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "#     # Lemmatize all words in documents.\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "#     return docs\n",
    "\n",
    "# docs = docs_preprocessor(corpus)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "\n",
    "# # Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "# bigram = Phrases(docs, min_count=10)\n",
    "# trigram = Phrases(bigram[docs])\n",
    "\n",
    "# for idx in range(len(docs)):\n",
    "#     for token in bigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             docs[idx].append(token)\n",
    "#     for token in trigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a trigram, add to document.\n",
    "#             docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5003fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# dictionary = Dictionary(docs)\n",
    "\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import LdaModel\n",
    "\n",
    "# # Set training parameters.\n",
    "# num_topics = 4\n",
    "# chunksize = 500 # size of the doc looked at every pass\n",
    "# passes = 20 # number of passes through documents\n",
    "# iterations = 400\n",
    "# eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# # Make a index to word dictionary.\n",
    "# temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "# id2word = dictionary.id2token\n",
    "\n",
    "# %time \n",
    "\n",
    "# model = LdaModel(corpus=corpus, id2word=id2word, \n",
    "#                  chunksize=chunksize, alpha='auto', \n",
    "#                  eta='auto', iterations=iterations, \n",
    "#                  num_topics=num_topics, passes=passes, \n",
    "#                  eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e06d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim_models \n",
    "# pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.gensim_models.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a8476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276a34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2ade6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
